{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Functions\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def reLU(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "def leakyReLU(x):\n",
    "    return max(0.1, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return (1 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 - np.square(np.tanh(x))\n",
    "\n",
    "# Loss Functions \n",
    "def logloss(y, a):\n",
    "    return -(y*np.log(a) + (1-y)*np.log(1-a))\n",
    "\n",
    "def d_logloss(y, a):\n",
    "    return (a - y)/(a*(1 - a))\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The layer class\n",
    "class Layer:\n",
    "\n",
    "    activationFunctions = {\n",
    "        'tanh': (tanh, d_tanh),\n",
    "        'sigmoid': (sigmoid, d_sigmoid)\n",
    "    }\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    def __init__(self, inputs, neurons, activation):\n",
    "        self.W = np.random.randn(neurons, inputs)\n",
    "        self.b = np.zeros((neurons, 1))\n",
    "        self.act, self.d_act = self.activationFunctions.get(activation)\n",
    "\n",
    "    def feedforward(self, A_prev):\n",
    "        self.A_prev = A_prev\n",
    "        self.Z = np.dot(self.W, self.A_prev) + self.b\n",
    "        self.A = self.act(self.Z)\n",
    "        return self.A\n",
    "\n",
    "    # forwardProp harusnya\n",
    "    def backprop(self, dA):\n",
    "        dZ = np.multiply(self.d_act(self.Z), dA)\n",
    "        dW = 1/dZ.shape[1] * np.dot(dZ, self.A_prev.T)\n",
    "        db = 1/dZ.shape[1] * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "\n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "\n",
    "        return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # dim x m\n",
    "y_train = np.array([[0, 1, 1, 0]]) # 1 x m\n",
    "\n",
    "m = 4\n",
    "epochs = 1500\n",
    "\n",
    "layers = [Layer(2, 3, 'tanh'), Layer(3, 1, 'sigmoid')]\n",
    "costs = [] # to plot graph \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    A = x_train\n",
    "    for layer in layers:\n",
    "        A = layer.feedforward(A)\n",
    "\n",
    "    cost = 1/m * np.sum(logloss(y_train, A))\n",
    "    costs.append(cost)\n",
    "\n",
    "    dA = d_logloss(y_train, A)\n",
    "    for layer in reversed(layers):\n",
    "        dA = layer.backprop(dA)\n",
    "\n",
    "\n",
    "# Making predictions\n",
    "A = x_train\n",
    "for layer in layers:\n",
    "    A = layer.feedforward(A)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs), costs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
